import os
import hashlib
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from datetime import datetime
from collections import defaultdict

# ====================
# –•–ê–†–î–ö–û–î-–ù–ê–°–¢–†–û–ô–ö–ò
# ====================
DATASET_PATHS = {
    "train": "dataset_split/train",
    "valid": "dataset_split/val",  # –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
    "test": "dataset_split/test"
}
HASH_ALGO = "md5"
OUTPUT_FILE = "cross_leakage_report.txt"

def get_file_hash(filepath: str) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à-—Å—É–º–º—É —Ñ–∞–π–ª–∞"""
    hash_func = getattr(hashlib, HASH_ALGO)()
    with open(filepath, 'rb') as f:
        while chunk := f.read(8192):
            hash_func.update(chunk)
    return hash_func.hexdigest()

def scan_all_datasets() -> dict:
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {—Ö—ç—à: {dataset_type: [–ø—É—Ç–∏]}}"""
    all_hashes = defaultdict(lambda: defaultdict(list))
    
    for dataset_type, dataset_path in DATASET_PATHS.items():
        if not os.path.exists(dataset_path):
            print(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {dataset_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç! –ü—Ä–æ–ø—É—Å–∫–∞—é...")
            continue
            
        files_to_process = []
        for root, _, files in os.walk(dataset_path):
            files_to_process.extend(os.path.join(root, f) for f in files)
        
        with ThreadPoolExecutor() as executor:
            futures = {executor.submit(get_file_hash, fp): fp for fp in files_to_process}
            for future in tqdm(futures, desc=f"–°–∫–∞–Ω–∏—Ä—É—é {dataset_type}", unit="file"):
                filepath = futures[future]
                file_hash = future.result()
                all_hashes[file_hash][dataset_type].append(filepath)
    
    return all_hashes

def analyze_leakages(all_hashes: dict) -> dict:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏"""
    leakages = defaultdict(list)
    
    for file_hash, datasets in all_hashes.items():
        if len(datasets) > 1:  # –§–∞–π–ª –µ—Å—Ç—å –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
            leakage = {
                "hash": file_hash,
                "locations": datasets
            }
            # –ö–ª—é—á –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä "train-test" –∏–ª–∏ "train-valid-test")
            datasets_key = "-".join(sorted(datasets.keys()))
            leakages[datasets_key].append(leakage)
    
    return leakages

def save_report(leakages: dict) -> None:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(f"–û—Ç—á—ë—Ç –æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è—Ö –≤ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö ({timestamp})\n\n")
        
        for dataset_key in DATASET_PATHS:
            f.write(f"{dataset_key}: {DATASET_PATHS[dataset_key]}\n")
        f.write(f"\n–ê–ª–≥–æ—Ä–∏—Ç–º —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {HASH_ALGO}\n")
        
        total_leakages = sum(len(v) for v in leakages.values())
        f.write(f"\n–í—Å–µ–≥–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π: {total_leakages}\n")
        
        for datasets_key, items in leakages.items():
            f.write(f"\n=== –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è {datasets_key} ({len(items)} —à—Ç.) ===\n")
            for i, item in enumerate(items, 1):
                f.write(f"\n–î—É–±–ª–∏–∫–∞—Ç #{i} (—Ö—ç—à: {item['hash']}):\n")
                for ds_type, paths in item['locations'].items():
                    f.write(f"–í {ds_type}:\n")
                    for path in paths:
                        f.write(f"  - {path}\n")
    
    print(f"\n‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_FILE}")

def main():
    print("üîç –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏")
    print("‚ñ∏ train:", DATASET_PATHS["train"])
    print("‚ñ∏ valid:", DATASET_PATHS["valid"])
    print("‚ñ∏ test:", DATASET_PATHS["test"])
    
    all_hashes = scan_all_datasets()
    leakages = analyze_leakages(all_hashes)
    
    if not leakages:
        print("\n‚úÖ –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ!")
        return
    
    print("\n‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è:")
    for datasets_key, items in leakages.items():
        print(f"‚ñ∏ {datasets_key}: {len(items)} —Ñ–∞–π–ª–æ–≤")
    
    save_report(leakages)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    unique_files = sum(len(paths) for paths in all_hashes.values())
    print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(all_hashes)}")
    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ —Å —É—á—ë—Ç–æ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {unique_files}")

if __name__ == "__main__":
    main()